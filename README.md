PS. Те замечния по коду и архитектуре скрипта пока не исправил и не дорааботал (можно не смотреть) начну деалть следом пока лишь собрал в кучу все добивл блок анализа данных он тоже лежит в репозитории в блоке буде на него ссыылкаа как и на визуализаию, также прописал методологию и струкктуру общего проектаб также пока не могу нормально добавить тетрадку тк она много весит а в пдф есть сложности с переносом изза библитеки plotly, пока просто прикрепляю ссылку на диск с файлом
# WB_data_analisys
## Содержание
- [Цель и задачи](#title1)
- [Сбор и подготовка данных](#title2)
- [Анализ данных](#title3)
## <a id="title1">Цель и задачи</a>
**Цель**

Целью данного исследования является комплексный анализ факторов, связанных с ожидаемой продолжительностью жизни населения в странах мира, а также выявление устойчивых социально-экономических, демографических и структурных различий между странами на разных этапах развития.

**Задачи**

- Парсинг данных API [World Bank](https://www.worldbank.org/ext/en/home)
- Обработка и нормализция полученных данных
- Организация хранилища для нормализоованных данных (облачная СУБД Postgresql [Supabase](https://supabase.com/))
- Подготовка витрин данных для BI системы
- Разведочный и структурный анализ (Python)
- Динамический анализ данных (Yandex Datalens)

**Итог**

Релизован ETL пайплайн, которые парсит данныые по API WB, обрабаытвает и нормализует их, и выгружает в развернутую облачную СУБД Postgres (Supabase). [Дашборд в Yandex Datalens](https://datalens.ru/l58lqbi9o6em5), который подключается к нашей CУБД, и берет за основу специально подготовеные [витрины данных](https://github.com/ososovyan/WB_data_analisys/blob/main/vew_preaparation.sql). И отдельно [разведочный и структурный в Python](https://disk.yandex.ru/d/gGTpmcA2onB23A)

**Стек**

ETL pipline : Python, requests, pandas, numpy, re, json, psycopg2 

Анализ данных : Jupyter Notebook, Python, pandas, matplotlib, seeaborn, plotly, sklearn, scipy, sqlalchemy (для подключения к БД)

BI: Postgresql, Yandex Datalens

## <a id="title2">Сбор и подготовка данных</a>
Сбор, обработка и загрузка данных осущетсвлена по логике ETL: процесс Extract – Transform – Load.
На вход наш скрипт получает `config.json` с параметрами подключения к API WB, а также параметрами для подключения к СУБД Postgresql.
Также наш скрипт осуществляет логирование

**0. Загрузка конфигурации**
Наш скрипт загружает данные конфигурцаии из файла, затем идет блок валидации данных полученных из этого файла, алгоритм валидации делает подключение к API болле робастным (отработка возможных ошибок, легких опечаток не нарушает работу скрипта, также выявление нарушения структуры или наличия ошибок можно отследить в log файле)

**1. Extract**

Данная часть отвечает за загрузку данных с APi. Мы используем библитоеку request для подключения к API. Релизована ассинхронная retry-логика: некоторые ошибки (неклиентсике) не останавливают процесс, происходит попытка переподключения (кол-во можно регулировать в config). Также релизована логика обработки пагинцаии. Обе функции универсаальны и подходят для работы с любыми API. Реализована отдельная функция, которая обрабатывает данные configa и создает настройки для подключения к API WB.

**2. Transform**

Данный блок посвящен обработке полученных данных их нормализации. В этом блоке мы приводим `json -> pandas`. Коллонки полученного датфрейма приводим к `snake_case`, убюираем лишние колонки (вложеные структуры). Мы ожидаем увидеть определенную структуру, поэтому на данныом этапе производим создание справочных таблиц, для организации в БД и оптимизации памяти, а также стандартизируем вид возмонжных pk и fk и названия справочных таблиц, для этго используем регулярные выражения (библитека re). 

**3. Load**

Данный блок посвящен работе с СУБД. На данном этапе создавется подключение к БД: c помощью библитеки psycopg2 мы создаем подключение к БД (параметры для подключения мы получем из валидированного config). Далее пред тем как выполнить основной запрос, мы проверяем наличие справочных таблиц, обновляем их, если их не существуетто создаем.      

![Схема хранения данных](https://github.com/ososovyan/WB_data_analisys/blob/main/Scema_bd.png) представлена на рисунке тип снежинка. Если справочных таблиц нет - мы идем в обратном порядке уровня. Мы создаем таблицу проверяем каакие таблицы уже существуют и по ожидаемому имени таблице, имени pk и fk автоматичсеки устанавливае связи внутри (Это происходило автоматически для каждой таблицы за счет функции, которая использует динамические sql запросы в зависмости от срдержимого pd.df. Функция универсальна не привязана к конкретной архитктуре, ограничена лишь логикой нейминга таблиц и ключей. Если таблица сущетсвует мы лишь обновляем и дописыываем содержимое pd.df.

## <a id="title3">Анализ данных</a>

Первый пайплайн create_ref_tables()
Наверное скажу что здесь реализовано два пайплайна и оба связаны с справочными таблицами
- Первый создает справочные таблицы стран, и справоные таблицы для этой таблицы create_ref_tables_con()
  в итоге создается четыре таблицы contry, region, adminregion, income_level, lending_type(подробнее на рисунке)
- Второй создает справочные таблицы индикаторов, и справочныые таблицыы для этой таблицы create_ref_tables_ind()
  в итоге создается всего две таблицы indicator, source (в дальнейшем можно реализовать создание справочной таблицы
  topic - на данном этапе не понятно нужна ли она и она требует отдельного вниаминия дополнений при развертке ответа от api
  
Реализован logger

На вход каждый скрипт принимаает ".../config.json" содержащий необходимые данные для создания подключения к Postgresss
а также для работы c api
Есть функция валидации cfg которая делает работу скрипта слегка робастной к мелким ошибкам ввода
Архитктура основного пфйплайна условно extract->transsform->load
Реализована функция безопасного запроса к api а также универсальная функкция отработки пагинации
Этап трансформации немного провалился в моем понимании много хард кода
Загрузку в бд совершаю используя библитеку psycopg2 тк решил использовать динамические sql запросы к бд
очень наприятно что при запросе к странам id страны iso3, а вот когда делать основной запрос то он решает теперь сдеелать его iso2 добаввил хардкода
![Схема бд](https://github.com/ososovyan/WB_data_analisys/blob/main/Scema_bd.png)

